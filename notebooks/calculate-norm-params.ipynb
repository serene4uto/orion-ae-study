{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d238f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# More reliable: get the project root from the notebook's location\n",
    "ROOT_PATH = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "# Or even better for notebooks:\n",
    "ROOT_PATH = Path().resolve().parent  # Goes up from notebooks/ folder\n",
    "\n",
    "sys.path.append(str(ROOT_PATH))\n",
    "\n",
    "from src.data.dataset import OrionAEFrameDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41bba2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = OrionAEFrameDataset(\n",
    "    data_path=r\"C:\\Users\\nguye\\Documents\\GitHub\\orion-ae-study\\data\\raw\\segmented_ms_30_0_o_0_00_c_A_B_C_D_20251213_092549\",\n",
    "    config_path=r\"C:\\Users\\nguye\\Documents\\GitHub\\orion-ae-study\\configs\\dataset\\example_1.yaml\",\n",
    "    type=\"train\"\n",
    ")\n",
    "\n",
    "val_set = OrionAEFrameDataset(\n",
    "    data_path=r\"C:\\Users\\nguye\\Documents\\GitHub\\orion-ae-study\\data\\raw\\segmented_ms_30_0_o_0_00_c_A_B_C_D_20251213_092549\",\n",
    "    config_path=r\"C:\\Users\\nguye\\Documents\\GitHub\\orion-ae-study\\configs\\dataset\\example_1.yaml\",\n",
    "    type=\"val\"\n",
    ")\n",
    "\n",
    "test_set = OrionAEFrameDataset(\n",
    "    data_path=r\"C:\\Users\\nguye\\Documents\\GitHub\\orion-ae-study\\data\\raw\\segmented_ms_30_0_o_0_00_c_A_B_C_D_20251213_092549\",\n",
    "    config_path=r\"C:\\Users\\nguye\\Documents\\GitHub\\orion-ae-study\\configs\\dataset\\example_1.yaml\",\n",
    "    type=\"test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10f2ef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2301"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc7479d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.295999 ,  -4.3946652,  -2.1973326, ...,  -9.887997 ,\n",
       "         -9.887997 ,  -7.6906643],\n",
       "       [  4.333628 ,   0.       ,  -3.295999 , ...,   4.333628 ,\n",
       "          7.56859  ,  10.86459  ],\n",
       "       [  5.432295 ,   2.1362956,   3.234962 , ..., -18.555254 ,\n",
       "        -20.69155  , -21.790216 ]], shape=(3, 150000), dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0]['raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bf47fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.04889 , 5.631505, 7.921509], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(test_set[0]['raw'], axis = 1)\n",
    "np.std(test_set[0]['raw'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c70df2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((np.array([1,2,3]), np.array([3,4,5])), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c033d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "def calculate_norm_params(\n",
    "    datasets: List,\n",
    "    target_label: int,\n",
    "    target_serie: str,\n",
    "    show_progress: bool = True\n",
    ") -> Tuple[Optional[np.ndarray], Optional[np.ndarray], dict]:\n",
    "    \"\"\"\n",
    "    Calculate mean and std normalization parameters for a specific label and series.\n",
    "    \n",
    "    Uses memory-efficient running sums to avoid storing all data in RAM.\n",
    "    \n",
    "    Args:\n",
    "        datasets: List of dataset objects (e.g., [train_set, val_set, test_set])\n",
    "        target_label: Target label to filter by\n",
    "        target_serie: Target series name to filter by\n",
    "        show_progress: Whether to show progress bars\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (means, stds, info_dict) where:\n",
    "            - means: Array of means per channel, or None if no items found\n",
    "            - stds: Array of stds per channel, or None if no items found\n",
    "            - info_dict: Dictionary with 'item_count' and 'total_count'\n",
    "    \"\"\"\n",
    "    num_channels = None\n",
    "    sum_values = None\n",
    "    sum_squared = None\n",
    "    total_count = 0\n",
    "    item_count = 0\n",
    "    \n",
    "    # Iterate through all datasets and accumulate statistics\n",
    "    for dataset in datasets:\n",
    "        iterator = dataset\n",
    "        if show_progress:\n",
    "            dataset_name = getattr(dataset, 'type', 'dataset')\n",
    "            iterator = tqdm(dataset, desc=f\"Processing {dataset_name}\", leave=False)\n",
    "        \n",
    "        for item in iterator:\n",
    "            if item['label'] == target_label and item['serie'] == target_serie:\n",
    "                raw_data = item['raw']  # Shape: (channels, time_steps)\n",
    "                \n",
    "                # Initialize accumulators on first item\n",
    "                if num_channels is None:\n",
    "                    num_channels = raw_data.shape[0]\n",
    "                    sum_values = np.zeros(num_channels, dtype=np.float64)\n",
    "                    sum_squared = np.zeros(num_channels, dtype=np.float64)\n",
    "                \n",
    "                # Accumulate statistics\n",
    "                sum_values += np.sum(raw_data, axis=1, dtype=np.float64)\n",
    "                sum_squared += np.sum(raw_data.astype(np.float64) ** 2, axis=1)\n",
    "                total_count += raw_data.shape[1]\n",
    "                item_count += 1\n",
    "    \n",
    "    # Compute final statistics with numerical stability\n",
    "    if total_count > 0:\n",
    "        means = sum_values / total_count\n",
    "        # More numerically stable variance calculation\n",
    "        variance = (sum_squared / total_count) - (means ** 2)\n",
    "        variance = np.maximum(variance, 0.0)  # Handle floating point errors\n",
    "        stds = np.sqrt(variance)\n",
    "        \n",
    "        info = {\n",
    "            'item_count': item_count,\n",
    "            'total_count': total_count,\n",
    "            'num_channels': num_channels\n",
    "        }\n",
    "        \n",
    "        return means, stds, info\n",
    "    else:\n",
    "        return None, None, {'item_count': 0, 'total_count': 0, 'num_channels': None}\n",
    "\n",
    "\n",
    "def calculate_norm_params_batch(\n",
    "    datasets: List,\n",
    "    target_labels: Optional[List[int]] = None,\n",
    "    target_series: Optional[List[str]] = None,\n",
    "    show_progress: bool = True\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calculate normalization parameters for multiple label/series combinations.\n",
    "    \n",
    "    Args:\n",
    "        datasets: List of dataset objects\n",
    "        target_labels: List of target labels (None = all labels)\n",
    "        target_series: List of target series (None = all series)\n",
    "        show_progress: Whether to show progress bars\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys like (label, serie) -> {'mean': array, 'std': array, 'info': dict}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Collect all unique labels and series if not specified\n",
    "    if target_labels is None or target_series is None:\n",
    "        seen_labels = set()\n",
    "        seen_series = set()\n",
    "        for dataset in datasets:\n",
    "            for item in dataset:\n",
    "                seen_labels.add(item['label'])\n",
    "                seen_series.add(item['serie'])\n",
    "        \n",
    "        if target_labels is None:\n",
    "            target_labels = sorted(seen_labels)\n",
    "        if target_series is None:\n",
    "            target_series = sorted(seen_series)\n",
    "    \n",
    "    # Calculate for each combination\n",
    "    for label in target_labels:\n",
    "        for serie in target_series:\n",
    "            means, stds, info = calculate_norm_params(\n",
    "                datasets=datasets,\n",
    "                target_label=label,\n",
    "                target_serie=serie,\n",
    "                show_progress=show_progress\n",
    "            )\n",
    "            results[(label, serie)] = {\n",
    "                'mean': means,\n",
    "                'std': stds,\n",
    "                'info': info\n",
    "            }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed236d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{(6, 'B'): {'mean': array([0.58009147, 2.13962315, 0.12031674]),\n",
       "  'std': array([4.78606225, 6.49078199, 7.51684796]),\n",
       "  'info': {'item_count': 305, 'total_count': 45750000, 'num_channels': 3}},\n",
       " (6, 'C'): {'mean': array([1.1293965 , 2.04018865, 0.27358819]),\n",
       "  'std': array([4.62223859, 7.79696241, 9.22351764]),\n",
       "  'info': {'item_count': 311, 'total_count': 46650000, 'num_channels': 3}},\n",
       " (6, 'D'): {'mean': array([0.22941354, 1.4716678 , 0.6410115 ]),\n",
       "  'std': array([4.81425054, 7.36753723, 9.13285726]),\n",
       "  'info': {'item_count': 306, 'total_count': 45900000, 'num_channels': 3}},\n",
       " (6, 'E'): {'mean': array([0.11833483, 1.12777588, 0.30956035]),\n",
       "  'std': array([4.89143049, 6.43525229, 9.54307939]),\n",
       "  'info': {'item_count': 310, 'total_count': 46500000, 'num_channels': 3}},\n",
       " (6, 'F'): {'mean': array([0.53612407, 2.11247939, 0.30080982]),\n",
       "  'std': array([4.88715638, 6.36863377, 8.54823905]),\n",
       "  'info': {'item_count': 329, 'total_count': 49350000, 'num_channels': 3}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_norm_params_batch(\n",
    "    datasets=[train_set, val_set, test_set],\n",
    "    target_labels=[6],\n",
    "    target_series=['B', 'C', 'D', 'E', 'F']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a5c5a",
   "metadata": {},
   "source": [
    "{(6, 'B'): {'mean': array([0.58009147, 2.13962315, 0.12031674]),\n",
    "  'std': array([4.78606225, 6.49078199, 7.51684796]),\n",
    "  'info': {'item_count': 305, 'total_count': 45750000, 'num_channels': 3}},\n",
    " (6, 'C'): {'mean': array([1.1293965 , 2.04018865, 0.27358819]),\n",
    "  'std': array([4.62223859, 7.79696241, 9.22351764]),\n",
    "  'info': {'item_count': 311, 'total_count': 46650000, 'num_channels': 3}},\n",
    " (6, 'D'): {'mean': array([0.22941354, 1.4716678 , 0.6410115 ]),\n",
    "  'std': array([4.81425054, 7.36753723, 9.13285726]),\n",
    "  'info': {'item_count': 306, 'total_count': 45900000, 'num_channels': 3}},\n",
    " (6, 'E'): {'mean': array([0.11833483, 1.12777588, 0.30956035]),\n",
    "  'std': array([4.89143049, 6.43525229, 9.54307939]),\n",
    "  'info': {'item_count': 310, 'total_count': 46500000, 'num_channels': 3}},\n",
    " (6, 'F'): {'mean': array([0.53612407, 2.11247939, 0.30080982]),\n",
    "  'std': array([4.88715638, 6.36863377, 8.54823905]),\n",
    "  'info': {'item_count': 329, 'total_count': 49350000, 'num_channels': 3}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc1200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Memory-efficient computation of statistics using running sums\n",
    "# # This avoids storing all data in RAM at once\n",
    "\n",
    "# target_label = 6\n",
    "# target_serie = 'B'\n",
    "# num_channels = None  # Will be determined from first item\n",
    "# sum_values = None    # Running sum per channel\n",
    "# sum_squared = None   # Running sum of squares per channel\n",
    "# total_count = 0      # Total number of time steps across all items\n",
    "# item_count = 0       # Number of items processed\n",
    "\n",
    "# # Iterate through all datasets and accumulate statistics\n",
    "# for dataset in [train_set, val_set, test_set]:\n",
    "#     for item in dataset:\n",
    "#         if item['label'] == target_label and item['serie'] == target_serie:\n",
    "#             # item['raw'] has shape (channels, time_steps)\n",
    "#             raw_data = item['raw']  # Shape: (channels, time_steps)\n",
    "            \n",
    "#             # Initialize accumulators on first item\n",
    "#             if num_channels is None:\n",
    "#                 num_channels = raw_data.shape[0]\n",
    "#                 sum_values = np.zeros(num_channels)\n",
    "#                 sum_squared = np.zeros(num_channels)\n",
    "            \n",
    "#             # Flatten along time_steps axis and accumulate\n",
    "#             # For each channel, sum all time_steps\n",
    "#             sum_values += np.sum(raw_data, axis=1)  # Sum across time_steps for each channel\n",
    "#             sum_squared += np.sum(raw_data ** 2, axis=1)  # Sum of squares\n",
    "#             total_count += raw_data.shape[1]  # Number of time_steps\n",
    "#             item_count += 1\n",
    "\n",
    "# # Compute final statistics\n",
    "# if total_count > 0:\n",
    "#     means = sum_values / total_count  # Mean per channel\n",
    "#     # Standard deviation: sqrt(E[X^2] - E[X]^2)\n",
    "#     stds = np.sqrt((sum_squared / total_count) - (means ** 2))\n",
    "    \n",
    "#     print(f\"Found {item_count} items with label {target_label}\")\n",
    "#     print(f\"Total time steps: {total_count}\")\n",
    "#     print(f\"Means per channel: {means}\")\n",
    "#     print(f\"Stds per channel: {stds}\")\n",
    "# else:\n",
    "#     print(f\"No items found with label {target_label}\")\n",
    "#     means, stds = None, None\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471ea9a9",
   "metadata": {},
   "source": [
    "Found 305 items with label 6 - B (305 items)\n",
    "Total time steps: 45750000\n",
    "Means per channel: [0.58009146 2.13962311 0.12031676]\n",
    "Stds per channel: [4.78606218 6.49078193 7.51684789]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c905e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
