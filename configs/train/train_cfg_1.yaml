train:
  run_dir: null

  experiment_name: null
  
  # Device to use for training ('cuda', 'cpu', or 'auto')
  device: "cuda"
  
  # Number of training epochs
  num_epochs: 10
  
  # Batch size for training and validation
  batch_size: 8

  # Number of workers for data loading
  num_workers: 4

  # Pin memory for data loading
  pin_memory: true
  
  # Loss function configuration (can be a list for multiple losses)
  criterion:
    # - name: "POM1bLoss"
    #   weight: 1.0
    #   params: null
    #     # Optional: specify class weights if needed
    #     # weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
    - name: "nn.CrossEntropyLoss"
      weight: 1.0
      params: null
        # Optional: specify class weights if needed
        # weight: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
  
  # Optimizer configuration (support only one optimizer)
  optimizer:
    name: "Adam"
    lr: 0.001
    params:
      weight_decay: 0.0001
      betas: [0.9, 0.999]
      eps: 1e-8
  
  # Optional: Learning rate scheduler configuration
  scheduler:
    name: "StepLR"  # or "CosineAnnealingLR", "ReduceLROnPlateau", etc.
    params:
      step_size: 30
      gamma: 0.1
  
  # Optional: Regularization
  l1_reg: 0.0  # L1 regularization coefficient

  # Optional: Early stopping
  early_stopping:
    patience: 10
    min_delta: 0.001
    monitor: "val/loss" # or "val/acc" 

  # Optional: Checkpointing
  checkpoint:
    monitor: "val/loss" # or "val/acc"
    save_best: true
    save_every_n_epochs: 10

  # Optional: TensorBoard logging
  logging:
    # Enable/disable specific loggers
    tensorboard:
      enabled: true
      log_interval: 10  # Log every N batches (optional)
      log_graph: true  # Log model computational graph (optional)
      log_weights: false  # Log weight histograms (can be expensive)
      log_images: false  # Log sample images (if applicable)
    
    # Common settings
    log_interval: 10  # Log metrics every N batches (if not specified per logger)
    log_epoch_interval: 1  # Log metrics every N epochs
  
  # Optional: Gradient clipping
  max_grad_norm: null # 1.0 
  
  # Optional: Enable Automatic Mixed Precision (AMP) for faster training
  amp: true  # Set to true to enable AMP (requires CUDA)

  
